{"cells":[{"cell_type":"markdown","metadata":{"id":"ngUB2pOzOzCe"},"source":["### Блок теоретических вопросов"]},{"cell_type":"markdown","metadata":{"id":"cX_twM8nOzCg"},"source":["*Почему считается, что деревья склонны к переобучению?*: \n","\n","1. Это было замечено эмпирически.\n","2. Потому что деревья очень мощны, то есть могут восстановить практически любую зависимость в данных.\n","3. Из-за связи деревьев с линейными моделями, то есть дерево – это, по сути, линейная модель над какими-то признаками.\n","\n","\n","\n","\n","\n","**Ответ: 2)** Факт, показанный как на практике и лекциях, так и в домашнем задании. Деревья разбивают сколько угодно сложным образом признаковое пространство на простые кусочки.\n","\n","___________________________________________"]},{"cell_type":"markdown","metadata":{"id":"Ppv_xa6YOzCg"},"source":["*Как снижение минимального количества объектов в листовой вершине повлияет на качество на обучающей и тестовой выборках*: \n","\n","1.\tОшибка на обучающей и тестовых выборках будет падать\n","2.\tОшибка на обучающей выборке будет уменьшаться, а на тестовой падать до какого-то момента, а затем увеливаться\n","3.\tНельзя точно определить\n","4.\tОшибка на тестовой выборке будет падать, насчет ошибки на обучающей выборке нельзя сказать точно\n","\n","\n","\n","\n","\n","**Ответ: 2)** Чем больше минимальное количество обхектов в листовой вершине, тем более \"общее\" разделение мы требуем от требования. То есть улавливание более общих, нежели шумящих и частных закономерностей.\n","\n","___________________________________________"]},{"cell_type":"markdown","metadata":{"id":"zXEWmDegOzCh"},"source":["*Представим, что у нас имеются 2 дерева, уже обученных на одних и тех же данных. Отличаются они значениями гиперпараметров. Для первого дерева значения следующие: максимальная глубина = 8, минимальное количество объектов в листовой вершине=7. Для второго: максимальная глубина = 15, минимальное количество объектов в листовой вершине=5. Какое из них, по вашему мнению, выдаст худшее качество на тестовой выборке?*: \n","\n","1.\tВторое, так как максимальная глубина значительно больше, дерево успеет подогнаться под выбросы и тд.\n","2.\tПервое, так как глубина недостаточно большая, данные могут оказаться сложными, дерево не успеет повторить зависимость. \n","3.\tВопрос некорректен, так как мы не знаем, как устроены данные. Нам может хватить глубины 8, а может не хватить и 15. \n","4.\tВторое, так как такая большая глубина и малое количество объектов в листе почти что гарантируют переобучение\n","\n","\n","\n","\n","\n","\n","**Ответ: 3)** В машинном обучении, в действительности, не бывает идеального рецепта или элексира. Каждая крупица данных требует индивидуального подхода и тонны экспериментов!\n","\n","___________________________________________"]},{"cell_type":"markdown","metadata":{"id":"980FWedOOzCh"},"source":["### Блок практики"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cl_nVVzpOzCh"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","pd.options.display.max_columns = 500"]},{"cell_type":"markdown","metadata":{"id":"iQL1r4NtOzCi"},"source":["### Загрузим датасет с машинами. Цель - верно восстанавливать для каждой из них цену продажи!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69JxHkLpOzCj"},"outputs":[],"source":["data = pd.read_csv('autos.csv')\n","\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvV5isiVOzCj"},"outputs":[],"source":["### Колонка с тергетом - \"selling price\"\n","\n","X = data.drop(\"selling_price\", axis=1)\n","y = data[\"selling_price\"]\n","\n","### Будем замерять MSLE!\n","### Поэтому прологарифмируем таргет\n","### А после оптимизируем MSE\n","\n","y = y.apply(np.log1p)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8FRRqlEOzCj"},"outputs":[],"source":["### Разделим выборку на трейн и тест!\n","\n","from sklearn.model_selection import train_test_split \n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"]},{"cell_type":"markdown","metadata":{"id":"jjD917E-OzCj"},"source":["__Задание__ \n","\n","Реализуйте свой MeanTargetEncoder с добавленем некоторого шума!\n","\n","Однажды в лекционном материале, обсуждая счетчики, мы говорили с вами о том, что из-за них модели могут переобучаться. Один из способов бороться с этим - валидировать расчеты среднего таргета (стратегия отложенной выборки / расчеты на кросс-валидации). Но есть еще проще!\n","\n","Можно просто к значению счетчика добавить случайный шум (зашумить данные)!\n","\n","Напомним, что рассчитываться новые признаки должны по такой формуле:\n","\n","$$\n","g_j = \\frac{\\sum_{i=1}^{l} [f_j(x) = f_j(x_i)]}{l} + C * \\epsilon\n","$$\n","\n","\n","\n","Пусть шум будет случайной величиной из нормального стандартного распределения, то есть $\\epsilon \\sim N(0, 1) $, а $ C = 0.006$."]},{"cell_type":"markdown","metadata":{"id":"Gz1OvvyxOzCk"},"source":["Создавая свой класс-трансформер, наследуйтесь от классов `BaseEstimator, TransformerMixin` из `sklearn.base`. Трансформер не должен модифицировать передаваемую ему выборку inplace, а все необходимые статистики нужно считать только по обучающей выборке в методе `fit`. Ваш трансформер должен принимать при инициализации список из категориальных признаков и список из числовых признаков. \n","\n","\n","Если для какого-то признака в тестовой выборке отсутствует значение, трансформер должен поставить там 0.\n","\n","На выходе должен получиться датасет того же размера с измененными категориальными признаками."]},{"cell_type":"markdown","source":["Класс MeanTargetEncoderNoise должен иметь следующую сигнатуру:"],"metadata":{"id":"0ve7S06YPJAg"}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class MeanTargetEncoderNoise(BaseEstimator, TransformerMixin):\n","    \n","    def __init__(self, categorical, numeric):              \n","        ### Your code is here\n","    \n","    def fit(self, X, y):\n","        ### Your code is here\n","\n","        return self\n","        \n","    def transform(self, df):\n","        ### Your code is here\n","        \n","        return temp"],"metadata":{"id":"vvylX0htPIeM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Разделите колонки на вещественные и категориальные. Приведите все категориальные колонки к типу `object`.\n","\n","Далее применим наш кодировщик к `X_train, X_test`, так же как например мы применяем `StandardScaler`, чтобы проверить работоспособность нашего класса. Установите зерно датчика случайный чисел `np.random.seed(1)`.\n","\n","После того, как вы изменили обучающую и тестовую выборки, сохраните первые 10 строк полученного промежуточного датафрейма обучающей выборки (`X_train`) в файл в формате csv с сепаратором `;`. Не забудьте индекс. Отправьте полученный файл в форму ниже.\n","\n","Список колонок которые должны быть в файле для сдачи:\n","```py\n","cols = [\n","    \"km_driven\",\n","    \"name\",\n","    \"year\",\n","    \"fuel\",\n","    \"seller_type\",\n","    \"transmission\",\n","    \"owner\"\n","]\n","```"],"metadata":{"id":"XY-eKtYFPSHx"}},{"cell_type":"markdown","source":["Сделаем все как в задании."],"metadata":{"id":"WXOv9-lOPWuZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"W6bTvgcPOzCk"},"outputs":[],"source":["### Разделим колонки на вещественные и категориальные\n","\n","object_cols = ['name', 'year', 'fuel', 'seller_type', 'transmission', 'owner']\n","num_cols = ['km_driven']\n","\n","X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7epRSfGGOzCk"},"outputs":[],"source":["### Применим ко всем категориальным колонкам object type\n","\n","X[object_cols] = X[object_cols].astype(object)\n","X_test[object_cols] = X_test[object_cols].astype(object)\n","X_train[object_cols] = X_train[object_cols].astype(object)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ckHP-HBdOzCk"},"outputs":[],"source":["np.random.seed(1)\n","\n","### Функция считает среднее и добавляет шум из стандартного нормального распределения\n","\n","def func1(x):\n","    return np.sum(x) / x.size + 0.006 * np.random.normal(loc = 0.0, scale = 1.0, size =1)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Guj05iXPOzCk"},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","import itertools\n","\n","### Реализуем класс, считающий средние значения по таргету\n","### И немного зашумляющий их\n","### Данная реализация не претендует на звание самой лучшей!\n","### Любые другие, работающие за адекватное время, тоже подойдут\n","### Советуем потыкать и разобраться, если самостоятельно во время \n","### Выполнения ДЗ не получилось!\n","\n","class MeanTargetEncoderNoise(BaseEstimator, TransformerMixin):\n","    \n","    def __init__(self, categorical, numeric):\n","        self.categorical = categorical\n","        self.numeric = numeric\n","    \n","    def fit(self, X, y):\n","\n","        X['y'] = y\n","        \n","        arr = []\n","        \n","        for i in self.categorical:\n","            \n","            temp = X.groupby(i).agg({'y':[func1]}).reset_index()\n","            arr.append((list(temp[i]), list(temp['y']['func1'])))\n","            \n","        \n","        self.arr = arr\n","\n","        return self\n","        \n","    def transform(self, df):\n","        \n","        arr = self.arr\n","        \n","        temp = pd.DataFrame()\n","        \n","        c = 0\n","        \n","        for i in self.categorical:\n","            \n","            setik = set(df[i].unique())\n","            setik.difference_update(set(arr[c][0]))\n","\n","            column = df[i].replace(arr[c][0], arr[c][1]).reset_index()[i]\n","            column = column.replace(list(setik), 0).reset_index()[i]\n","\n","            temp = pd.concat([temp, column], axis=1)\n","            \n","            c+=1        \n","\n","        temp = pd.concat([df[self.numeric].reset_index(drop=True), temp], axis=1)\n","        \n","        return temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nANqKF1OzCl"},"outputs":[],"source":["### Проверим работоспособность\n","\n","np.random.seed(1)\n","transformer = MeanTargetEncoderNoise(categorical=object_cols, numeric=num_cols)\n","\n","transformer.fit(X_train, y_train)\n","\n","train = transformer.transform(X_train)\n","test = transformer.transform(X_test)\n","\n","train.head(10)"]},{"cell_type":"markdown","source":["Сохраним файл, так как требуется в задании."],"metadata":{"id":"Qb2PTsq9Thdo"}},{"cell_type":"code","source":["train.head(10).to_csv(\"result_18_step_5.csv\", sep=\";\", index=False)"],"metadata":{"id":"ba5-yItAPc6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XKFITqHvOzCl"},"source":["Обучите несколько деревьев, перебирая максимальную глубину алгоритма из списка `max_depth_list`, а остальные параметры оставьте дефолтными. Выведите лучшее значение гиперпараметра. Постройте график зависимости MSLE на тестовой выборке от значения гиперпараметра. Воспользуйтесь `Pipeline` без `GridSearch`. Проделайте то же самое с `min_samples_split`, `min_impurity_decrease`, `max_leaf_nodes`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SekEzOApOzCl"},"outputs":[],"source":["### Зададим сетку параметров\n","\n","max_depth_list = [3, 5, 8, 12]\n","min_samples_split_list = [10, 50, 100, 500]\n","min_impurity_decrease_list = [0, 0.1, 0.15, 0.2]\n","max_leaf_nodes_list = [100, 200, 500]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vHOG3m8OzCl"},"outputs":[],"source":["### Найдем лучшую глубину (при прочих дефолтных параметрах)\n","\n","from sklearn.metrics import mean_squared_error as mse\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.pipeline import Pipeline\n","np.random.seed(1)\n","rmse1 = []\n","\n","\n","for max_depth in max_depth_list:\n","\n","    pipe = Pipeline([(\"custom_transformer\",\n","                  MeanTargetEncoderNoise(categorical=object_cols, numeric=num_cols)),\n","                  \n","                  \n","                 (\"decision_tree\", \n","                  DecisionTreeRegressor(max_depth=max_depth))])\n","\n","    pipe.fit(X_train, y_train)\n","    \n","    preds = pipe.predict(X_test)\n","\n","    rmse1.append(mse(preds, y_test, squared=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmQzgpkMOzCm"},"outputs":[],"source":["### Найдем лучшее минимальное количество объектов в вершине (при прочих дефолтных параметрах)\n","\n","rmse2 = []\n","np.random.seed(1)\n","\n","for min_samples_split in min_samples_split_list:\n","\n","    pipe = Pipeline([(\"custom_transformer\",\n","                  MeanTargetEncoderNoise(categorical=object_cols, numeric=num_cols)),\n","                  \n","                  \n","                 (\"decision_tree\", \n","                  DecisionTreeRegressor(min_samples_split=min_samples_split))])\n","\n","    pipe.fit(X_train, y_train)\n","    \n","    preds = pipe.predict(X_test)\n","\n","    rmse2.append(mse(preds, y_test, squared=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99GAa8QxOzCm"},"outputs":[],"source":["### Найдем лучшее максимальное количество листьев (при прочих дефолтных параметрах)\n","\n","rmse3 = []\n","np.random.seed(1)\n","\n","for max_leaf_nodes in max_leaf_nodes_list:\n","\n","    pipe = Pipeline([(\"custom_transformer\",\n","                  MeanTargetEncoderNoise(categorical=object_cols, numeric=num_cols)),\n","                  \n","                  \n","                 (\"decision_tree\", \n","                  DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes))])\n","\n","    pipe.fit(X_train, y_train)\n","    \n","    preds = pipe.predict(X_test)\n","\n","    rmse3.append(mse(preds, y_test, squared=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AqU_yLKJOzCm"},"outputs":[],"source":["### Найдем лучшее минимальное улучшения критерия качества \n","### при выборе предиката (при прочих дефолтных параметрах)\n","\n","rmse4 = []\n","np.random.seed(1)\n","\n","for min_impurity_decrease in min_impurity_decrease_list:\n","\n","    pipe = Pipeline([(\"custom_transformer\",\n","                  MeanTargetEncoderNoise(categorical=object_cols, numeric=num_cols)),\n","                  \n","                  \n","                 (\"decision_tree\", \n","                  DecisionTreeRegressor(min_impurity_decrease=min_impurity_decrease))])\n","\n","    pipe.fit(X_train, y_train)\n","    \n","    preds = pipe.predict(X_test)\n","\n","    rmse4.append(mse(preds, y_test, squared=True))"]},{"cell_type":"markdown","metadata":{"id":"ROPAK-SgOzCm"},"source":["Изобразим результаты качества в зависимости от изменения отдельных параметров"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeBU6IN3OzCn"},"outputs":[],"source":["plt.plot(max_depth_list, rmse1)\n","plt.title('msle от max_depth')\n","plt.xlabel('max_depth')\n","plt.ylabel('rmse');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdMLsc1NOzCn"},"outputs":[],"source":["plt.plot(min_samples_split_list, rmse2)\n","plt.title('msle от min_samples_split')\n","plt.xlabel('min_samples_split')\n","plt.ylabel('rmse');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2dpumAiOzCn"},"outputs":[],"source":["plt.plot(max_leaf_nodes_list, rmse3)\n","plt.title('msle от max_leaf_nodes')\n","plt.xlabel('max_leaf_nodes')\n","plt.ylabel('rmse');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xcqhmdqOzCo"},"outputs":[],"source":["plt.plot(min_impurity_decrease_list, rmse4)\n","plt.title('msle от min_impurity_decrease')\n","plt.xlabel('min_impurity_decrease')\n","plt.ylabel('rmse');"]},{"cell_type":"markdown","metadata":{"id":"82UwMsZHOzCo"},"source":["Подберите лучшую комбинацию параметров, используя `GridSearchCV` и набор массивов значений параметров из предыдущего задания. Для лучшей комбинации посчитайте MSLE на тестовой выборке. Получились ли лучшие параметры такими же, как если бы вы подбирали их по-отдельности при остальных гиперпараметрах по умолчанию (предыдущее задание)?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ij9gObwOzCo"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import TimeSeriesSplit\n","\n","param_grid = {\n","    \"decision_tree__max_depth\": [3, 5, 8, 12],\n","    \"decision_tree__min_samples_split\": [10, 50, 100, 500],\n","    \"decision_tree__min_impurity_decrease\": [0, 0.1, 0.15, 0.2],\n","    \"decision_tree__max_leaf_nodes\": [100, 200, 500]\n","}\n","np.random.seed(1)\n","\n","pipe = Pipeline([(\"custom_transformer\",\n","                  MeanTargetEncoderNoise(categorical=object_cols, numeric=num_cols)),\n","                  \n","                  \n","                 (\"decision_tree\", \n","                  DecisionTreeRegressor())])\n","\n","search = GridSearchCV(pipe, \n","                      param_grid, \n","                      cv=4,\n","                      scoring='neg_mean_squared_error',\n","                      verbose=10)\n","\n","search.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJ4JyuFuOzCo"},"outputs":[],"source":["### Замерим MSE на лучшей модели\n","\n","mse(search.best_estimator_.predict(X_test), y_test, squared=True)"]}],"metadata":{"interpreter":{"hash":"8db21564b35bbdf2f1295d2e540489014671416f5dc577a5b9d4ca56833a3713"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}
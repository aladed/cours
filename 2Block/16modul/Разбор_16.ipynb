{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50211022",
   "metadata": {},
   "source": [
    "### Блок теоретических вопросов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119b4c6",
   "metadata": {},
   "source": [
    "*Каково качество работы классификатора kNN при числе соседей равным единице на обучающей и тестовой выборках?*: \n",
    "\n",
    "1.\tКачество на обучающей выборке максимальное, так как единственный сосед – это сам объект, тогда как на тестовой минимальное из-за переобучения.\n",
    "2.\tКачество на обучающей выборке минимальное, так как решающая поверхность становится негладкой из-за переобучения, но при этом идеально классифицирует тестовую выборку.\n",
    "3.\tКачество на обучающей и тестовой выборках минимальное из-за того, что модель переобучилась на единственном соседе.\n",
    "4.\tКачество на обучающей и тестовой выборках максимальное, так как ближайший сосед – это лучший прогноз для нового объекта.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 1)** В действительности, при обучении KNN запоминает соседей из трейна. Если взять один экземпляр оттуда в случае $k=1$, то прогноз сформируется за счет его же таргета. Поэтому модель переобучается.\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5138d0",
   "metadata": {},
   "source": [
    "*Допустим у нас большое число соседей, и мы хотим взвешивать их по удаленности с помощью ядра. При каком размере парзеновского окна h дальние соседи будет иметь наименьший вес?*: \n",
    "\n",
    "1.\tПри размере окна, большем, чем расстояние до последнего соседа, тогда вес дальних соседей будет меньше единицы.\n",
    "2.\tПри размере окна, равным единице дальние соседи будут иметь тот же вес, что и близкие, сделать их значимость меньше невозможно при использовании ядра.\n",
    "3.\tПри размере окна, которое стремится к бесконечности, веса дальних соседей будут уменьшаться, так как расстояние в аргументе ядра будет делиться на все большее число.\n",
    "4.\tПри размере окна, которое стремится к нулю, веса дальних соседей также стремятся к нулю, тогда как близкие соседи становятся все более значимыми\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 4)** Можно показать чисто математически, но так же рассуждали об этом на практике, экспериментируя с параметром ширины окна $h$.\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2e88b",
   "metadata": {},
   "source": [
    "*Основным недостатком метода kNN является*: \n",
    "\n",
    "1.\tНевозможность гибкой настройки обучения или модификации метода, единственный гиперпараметр – это число соседей.\n",
    "2.\tПри разном масштабе признаков kNN не может быть применён.\n",
    "3.\tМетод kNN может работать только с Евклидовой метрикой.\n",
    "4.\tДолгое время предсказания на больших выборках или при проклятии размерности.\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 4)** Пояснения по каждому пункту:\n",
    "\n",
    "1. у kNN существует множество модификаций, мы уже знаем, что можно использовать различные ядра, например, Гауссово ядро, у которого есть свой настраиваемый параметр ширины окна. \n",
    "2. Проблема разных масштабов у признаков просто решается нормировкой данных или использованием взвешенной метрики. \n",
    "3. Нет, можно использовать, например, расстояние Минковского.\n",
    "4. Долгий подсчет расстояний действительно затрудняет использование метода на практике, но все же эта проблема частично решается с помощью более продвинутых алгоритмов (locality-sensitivity hashing).\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0cb89f",
   "metadata": {},
   "source": [
    "### Блок практики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef15512",
   "metadata": {},
   "source": [
    "# k Nearest Neighbors (kNN)\n",
    "Метод k ближайших соседей рассчитывает расстояния от нового объекта до всех объектов выборки и, отбирая k ближайших, предсказывает целевую переменную с помощью голосования (в случае классификации) или усреднения (в случае регрессии). \\\n",
    "При этом этот метод можно по-разному настраивать, например можно изменить способ подсчета расстояний или способ взвешивания соседей по расстоянию. \\\n",
    "Для начала скачаем датасет и визуализируем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec727f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Загрузим датасет\n",
    "\n",
    "circ = pd.read_csv('concertriccir2.csv')\n",
    "circ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01af0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Изобразим данные\n",
    "\n",
    "sns.scatterplot(circ['X'], circ['Y'], hue=circ['class']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e85e08",
   "metadata": {},
   "source": [
    "**Задание 1.** Обучите логистическую регрессию на данных и нарисуйте разделяюущую гиперплоскость (прямую) на рисунке с данными. Как линейный классификатор справился с задачей? Сделайте вывод исходя из геометрии данных и линии разделения. Какой accuracy выйдет на всей выборке (без валидации) (2б)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d1e80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### Обучим меодель\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(circ[['X', 'Y']], circ['class'])\n",
    "\n",
    "### Достанем свободный коэффициент из уравнения получившейся гиперплоскости\n",
    "b = logit.intercept_[0]\n",
    "\n",
    "### Достанем веса возле каждый фичи из уравнения получившейся гиперплоскости\n",
    "w1, w2 = logit.coef_.T\n",
    "\n",
    "### Представим эту плоскость в виде уравнения y = m * x + c\n",
    "### Для этого расчитаем их следующим образом:\n",
    "c = -b/w2\n",
    "m = -w1/w2\n",
    "\n",
    "### Нагенерим точек для изображения этой прямой y = m * x + c\n",
    "xd = np.array([-4, 8])\n",
    "yd = m*xd + c\n",
    "\n",
    "### Нанесем все на график с помощью pyplot\n",
    "plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "plt.fill_between(xd, yd, 8, color='tab:blue', alpha=0.2)\n",
    "plt.fill_between(xd, yd, -4, color='tab:orange', alpha=0.2)\n",
    "\n",
    "plt.title('LogReg Decision boundary.')\n",
    "sns.scatterplot(circ['X'], circ['Y'], hue=circ['class']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Замерим accuracy\n",
    "\n",
    "logit.score(circ[['X', 'Y']], circ['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f4e1f",
   "metadata": {},
   "source": [
    "**Задание 2.** Разделите обучающую выборку и обучите 6 моделей kNN с различным числом соседей из `all_k`, сохраните обученные классификаторы в список. \\\n",
    "Выведите accuracy на тренировочной и тестовой выборке для каждой модели. Каково оптимальное число соседей с точки зрения accuracy? (2б)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Разделим выборку\n",
    "\n",
    "X = circ[['X', 'Y']].values\n",
    "y = circ['class'].values.astype('int')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    shuffle=True, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315691d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "all_k = [1, 3, 10, 30, 40, 60]\n",
    "\n",
    "### Обучим классификаторы и замерим в каждом из случаев качество \n",
    "\n",
    "all_clfs, acc_test, acc_train = [], [], []\n",
    "\n",
    "for k in all_k:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n",
    "    all_clfs.append(clf)\n",
    "    \n",
    "    acc_train.append(np.mean(clf.predict(X_train) == y_train))\n",
    "    acc_test.append(np.mean(clf.predict(X_test) == y_test))\n",
    "all_clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1492ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Результат в виде матрицы\n",
    "\n",
    "pd.DataFrame({'k' : all_k,\n",
    "              'ACC Train' : acc_train,\n",
    "              'ACC Test' : acc_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f814167",
   "metadata": {},
   "source": [
    "**Задание 3.** (БОНУС! БЕЗ БАЛЛОВ И ЧЕКЕРОВ!) Скачайте библиотку [mlxtend](http://rasbt.github.io/mlxtend/#examples) для визуализации решающих поверхностей. \n",
    "В документации можно ознакомиться с [примерами](http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/) изображения решающих поверхностей для различных моделей. \\\n",
    "Построим несколько таких графиков для шести обученных нами kNN.\n",
    "\n",
    "1. Подберите параметры `gridspec.GridSpec()` и `itertools.product()` для нашего числа классификаторов. \n",
    "2. Удобно перед визулизацией сохранить обученные классификаторы в список `all_clfs`.\n",
    "3. Проявите терпение! Отрисовка решающих поверхностей может занимать пару минут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9581b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mlxtend\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "\n",
    "%%time\n",
    "gs = gridspec.GridSpec(2, 3)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "labels = ['k = 1', 'k = 3', 'k = 10', 'k = 30', 'k = 40', 'k = 60']\n",
    "for clf, lab, grd in zip(all_clfs,\n",
    "                         labels,\n",
    "                         itertools.product([0, 1], [0, 1, 2])):\n",
    "    ax = plt.subplot(gs[grd])\n",
    "    fig = plot_decision_regions(X_train, y_train, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a277c052",
   "metadata": {},
   "source": [
    "1. Как меняется решающая поверхность с ростом числа соседей?\n",
    "2. Какое оптимальное число соседей с точки зрения устройства данных и геометрии решающих поверхностей? Поясните свой ответ, опираясь на полученные графики.\n",
    "3. Лучше ли справляется kNN по сравнению с логистической регрессией?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ca08e",
   "metadata": {},
   "source": [
    "**Задание 4.** Скачайте уже известный вам датасет [SigmaCabs](https://www.kaggle.com/datasets/arashnic/taxi-pricing-with-mobility-analytics). Обучите классификатор kNN на отнормированных и не отнормированных данных, подобрав лучшие гиперпараметры среди $k \\in [1, 5, 10]$. Замерьте качество на тестовой выборке. \\\n",
    "Почему нормирование данных помогает улучшить качество?\n",
    "\n",
    "-- Потому что учитывают масштаб фичей при расчете растояний\n",
    "\n",
    "(4б)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec72b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed_sigma_cabs.csv')\n",
    "df.head()\n",
    "\n",
    "X = df.drop('Surge_Pricing_Type', axis=1)\n",
    "y = df['Surge_Pricing_Type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    shuffle=True, random_state=2022)\n",
    "\n",
    "%%time\n",
    "all_k = [1, 5, 10]\n",
    "acc_test = []\n",
    "\n",
    "for k in all_k:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n",
    "    acc_test.append(np.mean(clf.predict(X_test) == y_test))\n",
    "    \n",
    "pd.DataFrame({'k' : all_k,\n",
    "              'ACC Test' : acc_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Теперь на отнормированных данных\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xsc_train = scaler.fit_transform(X_train)\n",
    "Xsc_test = scaler.transform(X_test)\n",
    "\n",
    "%%time\n",
    "all_k = [1, 5, 10]\n",
    "acc_test = []\n",
    "\n",
    "for k in all_k:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k).fit(Xsc_train, y_train)\n",
    "    acc_test.append(np.mean(clf.predict(Xsc_test) == y_test))\n",
    "    \n",
    "pd.DataFrame({'k' : all_k,\n",
    "              'ACC Test' : acc_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8ffb2",
   "metadata": {},
   "source": [
    "**Задание 5.** Обучите классификатор kNN на отнормированных данных с помощью метрики Минковского. \n",
    "$$\n",
    "\\rho(x, z) = \\bigg(\\sum_i |x_i - z_i|^p\\bigg) ^{1/p}\n",
    "$$\n",
    "\n",
    "Значение параметра `p` возьмите равным единице. Замерьте качество на тестовой выборке и сравните с предыдущим заданием, где `p = 2`. (2б)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4880f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = KNeighborsClassifier(n_neighbors=10, \n",
    "                           metric='minkowski',   ### <- указываем метрику Минковского\n",
    "                           p=1).fit(Xsc_train, y_train)\n",
    "\n",
    "print(f'ACC Test, (p = 1): {np.mean(clf.predict(Xsc_test) == y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1d9f6",
   "metadata": {},
   "source": [
    "**Задание 6.** Скачайте датасет [California Housing Prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices), в котором мы будем предсказывать среднюю стоимость жилья. В качестве признаков будем использовать `['longitude', 'latitude']`, посмотрите на них с помощью scatterplot. \\\n",
    "По какой характеристике наш kNN регрессор в этом случае подбирает соседей?\n",
    "\n",
    "1. Обучите обыкновенную линейную регрессию и замерьте RMSE на тренировочной и тестовой выборках.\n",
    "2. Обучите kNN регрессор на отнормированных данных, подобрав лучшие гиперпараметры. \n",
    "3. Обучите kNN регрессор с гауссовским ядром, подобрав оптимальное значение ширины окна среди предложенных $h \\in np.arange(0.02, 0.1, 0.01)$.\n",
    "\n",
    "Как ширина окна влияет на веса при усреднении ответа по соседям? \n",
    "\n",
    "(6б)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d54c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Загрузим данные и разобьем выборку\n",
    "\n",
    "df = pd.read_csv('housing.csv')\n",
    "df.head()\n",
    "\n",
    "X = df[['longitude', 'latitude']] # df.drop(['ocean_proximity', 'median_house_value'], axis=1)\n",
    "y = df['median_house_value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    shuffle=True, random_state=2022)\n",
    "sns.scatterplot('longitude', 'latitude', data=X, hue=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af9f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Отнормируем\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061025b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Замерим базовое качество Линейной Регрессии\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "print(f'Linreg Train RMSE: {mean_squared_error(y_train, lr.predict(X_train), squared=False).round(2)}')\n",
    "print(f'Linreg Test RMSE: {mean_squared_error(y_test, lr.predict(X_test), squared=False).round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bce78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Пробежимся по различным k\n",
    "\n",
    "all_k = np.arange(1, 10)\n",
    "rmse_test, rmse_train = [], []\n",
    "\n",
    "for k in all_k:\n",
    "    clf = KNeighborsRegressor(n_neighbors=k).fit(X_train, y_train)\n",
    "    \n",
    "    rmse_train.append(mean_squared_error(y_train, clf.predict(X_train), squared=False).round(2))\n",
    "    rmse_test.append(mean_squared_error(y_test, clf.predict(X_test), squared=False).round(2))\n",
    "    \n",
    "pd.DataFrame({'k' : all_k,\n",
    "              'RMSE Train' : rmse_train,\n",
    "              'RMSE Test' : rmse_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6979e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Замерим, как меняется качество на трейне и на тесте\n",
    "### При разной ширине окна в случае Гаусова Ядра\n",
    "\n",
    "rmse_test, rmse_train = [], []\n",
    "\n",
    "for h in np.arange(0.02, 0.1, 0.01):\n",
    "    def gaussian_kernel(distances, h=h):\n",
    "        return np.exp(- distances**2 / h**2)\n",
    "    \n",
    "    knn = KNeighborsRegressor(n_neighbors=7, weights=gaussian_kernel)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    rmse_train.append(mean_squared_error(y_train, knn.predict(X_train), squared=False).round(2))\n",
    "    rmse_test.append(mean_squared_error(y_test, knn.predict(X_test), squared=False).round(2))\n",
    "    \n",
    "    \n",
    "pd.DataFrame({'h' : np.arange(0.02, 0.1, 0.01),\n",
    "              'RMSE Train' : rmse_train,\n",
    "              'RMSE Test' : rmse_test})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
